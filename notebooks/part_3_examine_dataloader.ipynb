{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "026bbb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.getcwd() /home/carson/Desktop/Projects/language_reinforce/notebooks\n",
      "3.7.5 (default, Dec  9 2021, 17:04:37) \n",
      "[GCC 8.4.0]\n",
      "torch.__version__ 1.12.1+cu102\n",
      "0 cuda\n",
      "transformers.__version__ 4.22.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print('os.getcwd()', os.getcwd())\n",
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "print(sys.version)\n",
    "import time\n",
    "\n",
    "#plotting tools\n",
    "from matplotlib import pyplot as plt \n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "#torch libs\n",
    "import torch\n",
    "print('torch.__version__', torch.__version__)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pipe_device = 0 if torch.cuda.is_available() else -1\n",
    "print(pipe_device, device)\n",
    "\n",
    "#huggingface transformers\n",
    "import transformers\n",
    "print('transformers.__version__',transformers.__version__)\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2PreTrainedModel\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "#curious\n",
    "from curious.models import GPT2HeadWithValueModel\n",
    "from curious.rl import PPOTrainer\n",
    "from curious.utils import LengthSampler, collater, respond_to_batch, generate_text\n",
    "\n",
    "#jupyter stuff\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48b48dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/carson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load imdb with datasets\n",
    "ds = load_dataset('imdb', split='train')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcdb0e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />\",\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e72f9af",
   "metadata": {},
   "source": [
    "```\n",
    "Found cached dataset imdb (/home/carson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n",
    "Loading cached processed dataset at /home/carson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-fa8f4f047f540716.arrow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d762a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/carson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-fa8f4f047f540716.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['review', 'sentiment'],\n",
       "    num_rows: 24895\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename teh columns\n",
    "ds = ds.rename_columns({'text': 'review', 'label': 'sentiment'})\n",
    "# \n",
    "ds = ds.filter(lambda x: len(x[\"review\"])>200, batched=False)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56d28203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "528"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds[2]['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b615e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n",
      "4\n",
      "7\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "input_size = LengthSampler(min_value = 4, max_value = 8)\n",
    "output_size = LengthSampler(min_value = 4, max_value = 16)\n",
    "\n",
    "for i in range(10):\n",
    "    print(input_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b67e375",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'gpt2',\n",
    "    pad_token='<|endoftext|>',\n",
    "    padding_side = 'left',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac14f165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/carson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-a28a7f499415c40e.arrow\n"
     ]
    }
   ],
   "source": [
    "def map_tokenize(sample):\n",
    "    \n",
    "    '''\n",
    "    this function is applied to the dataset and \n",
    "    only the first few tokens of review are used for \"tokens\"\n",
    "    they are decoded and stored as query in their text form\n",
    "    '''\n",
    "    \n",
    "    sample[\"tokens\"] = gpt2_tokenizer.encode(sample[\"review\"])[:input_size()]\n",
    "    sample[\"query\"] = gpt2_tokenizer.decode(sample[\"tokens\"])\n",
    "    \n",
    "    return sample\n",
    "\n",
    "ds = ds.map(map_tokenize, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29741d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': \"If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />\",\n",
       " 'sentiment': 0,\n",
       " 'tokens': [1532, 691, 284, 3368],\n",
       " 'query': 'If only to avoid'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbc4a337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collater(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(ds, batch_size=32, collate_fn=collater)\n",
    "\n",
    "type(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "404890bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dataloader,'../small_data/dataloader.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51db6f09",
   "metadata": {},
   "source": [
    "This is Mapping Type DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "277cf1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> dict_keys(['review', 'sentiment', 'tokens', 'query'])\n",
      "['I rented I AM CURI', '\"I Am Curious: Yellow\"']\n",
      "[[40, 26399, 314, 3001, 327, 47269], [1, 40, 1703, 44269, 25, 12550, 1]]\n"
     ]
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(ds, batch_size=2, collate_fn=collater)\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "print(type(batch), batch.keys())\n",
    "print(batch['query'])\n",
    "print(batch['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4080c7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([   40, 26399,   314,  3001,   327, 47269], device='cuda:0'),\n",
       " tensor([    1,    40,  1703, 44269,    25, 12550,     1], device='cuda:0')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_tensors = [torch.tensor(t).long().to(device) for t in batch[\"tokens\"]]\n",
    "query_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ad435e",
   "metadata": {},
   "source": [
    "Later on this will be used in the code below during reinforcement learning\n",
    "\n",
    "```python\n",
    "\n",
    "input_ids = self.data_collator(\n",
    "    [torch.cat([q, r]) for q, r in zip(query_batch, response_batch)]\n",
    ")[\"input_ids\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits, _, v = self.model(input_ids)\n",
    "    ref_logits, _, _ = self.ref_model(input_ids)\n",
    "    \n",
    "logprobs = logprobs_from_logits(logits[:,:-1,:], input_ids[:,1:])\n",
    "ref_logprobs = logprobs_from_logits(ref_logits[:,:-1,:], input_ids[:,1:])\n",
    "\n",
    "```\n",
    "\n",
    "In a Data collator used for language modeling. Inputs are dynamically padded to the maximum length of a batch if they are not all of the same length.\n",
    "\n",
    "Notice below how in the 'input_ids' the padding is used to make the token sequence length match within the batch, but that in 'labels', the padding is set to -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62c72731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(gpt2_tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6fdf748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'labels'])\n",
      " \n",
      "tensor([[50256,    40, 26399,   314,  3001,   327, 47269],\n",
      "        [    1,    40,  1703, 44269,    25, 12550,     1]], device='cuda:0')\n",
      " \n",
      "tensor([[ -100,    40, 26399,   314,  3001,   327, 47269],\n",
      "        [    1,    40,  1703, 44269,    25, 12550,     1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "collated_batch = data_collator(\n",
    "    query_tensors\n",
    ")\n",
    "\n",
    "print(collated_batch.keys())\n",
    "print(' ')\n",
    "print(collated_batch['input_ids'])\n",
    "print(' ')\n",
    "print(collated_batch['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e59e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
